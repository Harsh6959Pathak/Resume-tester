import pandas as pd
import re
import ast
import numpy as np
import nltk
import torch
import openai
from google.colab import files
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Download NLTK stopwords
nltk.download('stopwords')

# File Upload (Google Colab)
uploaded = files.upload()

# Load dataset
dataset_filename = list(uploaded.keys())[0]  # Get uploaded filename
df = pd.read_csv(dataset_filename)
print(f"âœ… Successfully loaded: {dataset_filename}")
display(df.head())  # Show first few rows of dataset

# Preprocessing Function
def clean_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    return text

# Ensure 'skills' column is properly formatted
if 'skills' in df.columns:
    df['skills'] = df['skills'].apply(lambda x: ' '.join(ast.literal_eval(x)) if isinstance(x, str) else '')

# Apply cleaning to relevant columns
columns_to_clean = ['career_objective', 'degree_names', 'major_field_of_studies', 'skills', 'positions', 'responsibilities']
for col in columns_to_clean:
    if col in df.columns:
        df[col] = df[col].apply(clean_text)

# Combine features for resume text
df['resume_text'] = df[columns_to_clean].agg(' '.join, axis=1)

# Load Job Description (Mock JD Example)
jd = """[Job Description Text]"""

# Preprocess JD
jd_cleaned = clean_text(jd)

# TF-IDF Vectorization
tfidf = TfidfVectorizer()
resume_tfidf = tfidf.fit_transform(df['resume_text'])
jd_tfidf = tfidf.transform([jd_cleaned])

# Compute Cosine Similarity
cosine_scores = cosine_similarity(jd_tfidf, resume_tfidf)[0]
df['similarity_score'] = cosine_scores

# Upgrade Model for Better Accuracy
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)
resume_embeddings = bert_model.encode(df['resume_text'].tolist(), convert_to_tensor=True)
jd_embedding = bert_model.encode([jd_cleaned], convert_to_tensor=True)
bert_scores = cosine_similarity(jd_embedding.cpu().numpy(), resume_embeddings.cpu().numpy())[0]
df['bert_score'] = bert_scores

# Final Score (Weighted Combination)
df['final_score'] = 0.5 * df['similarity_score'] + 0.5 * df['bert_score']

# Skill Gap Analysis (Move BEFORE selecting top candidates)
required_skills = ["Python", "Machine Learning", "Cloud Computing"]
df['missing_skills'] = df['skills'].apply(lambda x: [skill for skill in required_skills if skill not in x.split()])

# Select Top 3 Candidates (AFTER Skill Gap Analysis)
top_candidates = df.nlargest(3, 'final_score')

# OpenAI GPT Integration for Ranking (Requires OpenAI API Key)
def rank_resume(resume_text, jd_text):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert HR recruiter."},
                {"role": "user", "content": f"Rank this resume based on its suitability for the job:\n{resume_text}\n Job Description: {jd_text}"}
            ]
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"Error: {e}"

df['gpt_rank'] = df['resume_text'].apply(lambda x: rank_resume(x, jd_cleaned))

# Display Results in Google Colab
print("\nðŸ“Œ **Top Candidates:**")
display(top_candidates[['resume_text', 'final_score', 'missing_skills']])

print("âœ… Process Completed Successfully!")
