import gradio as gr
import pandas as pd
import re
import numpy as np
import nltk
import torch
import time
import traceback
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Download stopwords
nltk.download('stopwords')

# Function to clean text
def clean_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    return text

# Resume Processing Function
def process_resumes(file, job_description):
    start_time = time.time()  # Start timer
    
    try:
        if file is None:
            return "‚ö†Ô∏è Please upload a valid CSV file!"

        df = pd.read_csv(file)  

        required_columns = {'skills', 'career_objective', 'degree_names', 'major_field_of_studies', 'positions', 'responsibilities'}
        if not required_columns.issubset(set(df.columns)):
            return f"‚ö†Ô∏è Missing required columns! Expected: {required_columns}, Found: {set(df.columns)}"

        # Clean text columns
        for col in required_columns:
            df[col] = df[col].apply(clean_text)

        # Combine resume text
        df['resume_text'] = df[list(required_columns)].agg(' '.join, axis=1)

        # Clean job description
        jd_cleaned = clean_text(job_description)

        # TF-IDF Vectorization
        tfidf = TfidfVectorizer()
        resume_tfidf = tfidf.fit_transform(df['resume_text'])
        jd_tfidf = tfidf.transform([jd_cleaned])

        # Compute Cosine Similarity
        cosine_scores = cosine_similarity(jd_tfidf, resume_tfidf)[0]
        df['similarity_score'] = cosine_scores

        # Load BERT Model
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        bert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)

        resume_embeddings = bert_model.encode(df['resume_text'].tolist(), convert_to_tensor=True, device=device)
        jd_embedding = bert_model.encode([jd_cleaned], convert_to_tensor=True, device=device)

        # Compute BERT Cosine Similarity
        bert_scores = cosine_similarity(jd_embedding.cpu().numpy(), resume_embeddings.cpu().numpy())[0]
        df['bert_score'] = bert_scores

        # Final Score Calculation
        df['final_score'] = 0.5 * df['similarity_score'] + 0.5 * df['bert_score']

        # Get Top 3 Candidates
        top_candidates = df.nlargest(3, 'final_score')[['resume_text', 'final_score']]

        # Stop timer
        end_time = time.time()
        elapsed_time = round(end_time - start_time, 2)

        # Format output as Markdown
        output = f"### üìå Processed in {elapsed_time} sec\n\n"
        output += "| Rank | Resume Preview | Final Score |\n|------|---------------|-------------|\n"
        for i, row in top_candidates.iterrows():
            preview_text = row['resume_text'][:100] + "..."  # Limit preview length
            output += f"| {i+1} | {preview_text} | {row['final_score']:.4f} |\n"

        return output

    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}\n\n```\n{traceback.format_exc()}\n```"

# Gradio UI
with gr.Blocks() as interface:
    gr.Markdown("# üìå Resume Ranking App")

    with gr.Row():
        file_input = gr.File(label="üìÇ Upload CSV File")
        job_desc_input = gr.Textbox(label="üìã Job Description", placeholder="Enter job description here...", lines=5)

    submit_button = gr.Button("üîç Rank Resumes")

    output_md = gr.Markdown()

    submit_button.click(process_resumes, inputs=[file_input, job_desc_input], outputs=output_md)

# Launch Web App
interface.launch(share=True)
